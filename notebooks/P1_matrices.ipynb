{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the end of this tutorial, you should be able to\n",
    "\n",
    "* Find the eigenvalue/eigenvector pairs of a given matrix\n",
    "* Find the inverse of a given matrix\n",
    "* Find the matrix exponential\n",
    "* Perform singular value decomposition\n",
    "* Find the pseudoinverse of a rectangular matrix\n",
    "\n",
    "There are five levels of comprehension that we will test in this class:\n",
    "* Can I describe it? (descriptive)\n",
    "* Can I express it mathematically? (theoretical)\n",
    "* Can I do a hand calculation? (practical)\n",
    "* Can I code it? (computational)\n",
    "* Do I know when to use it? (application)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the eigenvalues/vectors of a given matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the eigvenvalues and eigenvectors of a given matrix is one of the common operations with matrix math. Conceptually, For every square matrix $A$ there exists a number of scalar values $\\lambda$ (an eigenvalue), that, when multiplied by a corresponding vector $\\overrightarrow{\\eta}$ will return the same vector:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$A\\overrightarrow{\\eta} = \\lambda \\overrightarrow{\\eta} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question is: how do we find what these pair of eigvenvalues and eigenvectors for a given matrix? Doing a little rearranging gives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$(A-\\lambda I) \\overrightarrow{\\eta} = \\overrightarrow{0} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, unless $\\overrightarrow{\\eta} = \\overrightarrow{0}$ (which we don't want), then $(A-\\lambda I)$ must be equal to zero. This can only happen when the matrix is singular (e.g. its determinant is equal to zero). Thus, the eigenvalues of a given matrix can be found by the equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\det{A-\\lambda I} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be written out like so for an example 3 x 3 matrix:\n",
    "\n",
    "$$\n",
    "  \\det{\n",
    "  \\left[ {\\begin{array}{ccc}\n",
    "   3 - \\lambda & 2 & 4\\\\\n",
    "   2 & 0 - \\lambda & 2\\\\\n",
    "   4 & 2 & 3 - \\lambda\\\\\n",
    "  \\end{array} } \\right]} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing this out gives the following polynomial:\n",
    "    \n",
    "$$-\\lambda^3 + 6 \\lambda^2 + 15 \\lambda + 8 = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have a difficult time finding the roots to this equation. Conveniently, this matrix happens to be easy to solve using the Rational Roots Theorem. All the roots for this given matrix are rational:\n",
    "\n",
    "$$(\\lambda + 1)^2 (\\lambda - 8) = 0 $$\n",
    "\n",
    "Giving the eigenvalues $\\lambda = -1, -1, 8 $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvectors can then be found using row operations to convert the extended matrix into row echelon form after substituting in a given eigenvalue:\n",
    "\n",
    "$$\n",
    "  \\left[ {\\begin{array}{cccc}\n",
    "   4 & 2 & 4 & 0\\\\\n",
    "   2 & 1 & 2 & 0\\\\\n",
    "   4 & 2 & 4 & 0\\\\\n",
    "  \\end{array} } \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But uh oh... it looks like all rows aren't linearly independent. In this case, there are actually an infinite possible amount of solutions, but all solutions can be expressed as:\n",
    "\n",
    "$$y = -2x - 2z $$\n",
    "\n",
    "Thus, all potential eigenvectors can be expressed as:\n",
    "\n",
    "$$\\langle s, -2s - 2t, t \\rangle $$\n",
    "\n",
    "The first one, we can just pick two easy numbers, say, s=0 and t=1: $<0, -2, 1>$. The second must be orthogonal to the first, which can be ensured using the dot product:\n",
    "\n",
    "$$\\langle 0, -2, 1 \\rangle \\cdot \\langle s, -2s - 2t, t \\rangle = 0$$\n",
    "\n",
    "which gives the equation:\n",
    "\n",
    "$$4s + 5t = 0 $$\n",
    "\n",
    "This also has potentially infinite solutions, but we can pick s=-5 and t=4, giving $\\langle -5, 2, 4 \\rangle$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final eigenvalue can be found by substituting $\\lambda = 8$:\n",
    "\n",
    "$$\n",
    "  \\left[ {\\begin{array}{cccc}\n",
    "   -5 & 2 & 4 & 0\\\\\n",
    "   2 & -8 & 2 & 0\\\\\n",
    "   4 & 2 & -5 & 0\\\\\n",
    "  \\end{array} } \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also doesn't have a single solution, but we can fix one value arbitrarily (say y=1) and solve the system of equations to give the resulting eigenvector. After some math, it can be found that all potential vectors are of the form: $\\langle 2r, r, 2r \\rangle$. We can pick r=1 to give $\\langle 2, 1, 2 \\rangle$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing our eigenvectors to unit vectors give the following eigenvalue/eigenvector sets:\n",
    "    \n",
    "$$\\lambda = -1: \\langle 0, \\frac{-2}{\\sqrt{5}}, \\frac{1}{\\sqrt{5}} \\rangle$$\n",
    "$$\\lambda = -1: \\langle \\frac{-5}{\\sqrt{45}}, \\frac{2}{\\sqrt{45}}, \\frac{4}{\\sqrt{45}} \\rangle$$\n",
    "$$\\lambda = 8: \\langle \\frac{2}{3}, \\frac{1}{3}, \\frac{2}{3} \\rangle$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found a valid set of eigenvectors for the given matrix. That was a lot of work! And in most workflows, you probably aren't going to be solving these by hand. So let's try using a bit of Python to find the eigenvalue/vector pairs for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import eig\n",
    "A = np.array([[3,2,4],[2,0,2],[4,2,3]])\n",
    "u, v = eig(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalue: -1.0+0.0j and eigenvector: [-0.74535599  0.2981424   0.59628479]\n",
      "Eigenvalue: 8.0+0.0j and eigenvector: [0.66666667 0.33333333 0.66666667]\n",
      "Eigenvalue: -1.0+0.0j and eigenvector: [-0.20756326 -0.77602137  0.59557394]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print('Eigenvalue: {0:.1f} and eigenvector: {1}'.format(u[i], v[:, i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that their eigenvector aren't exactly the same as ours. The only one that is spot on is the eigenvector associated with $\\lambda=8$. But they should meet the criteria specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This eigenvector meets the specified criteria\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "s = v[0, i]\n",
    "t = v[2, i]\n",
    "try:\n",
    "    np.testing.assert_almost_equal(-2*s - 2*t, v[1,i])\n",
    "    print('This eigenvector meets the specified criteria')\n",
    "except:\n",
    "    print('Something is wrong with the specified eigvector')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary:\n",
    "\n",
    "* Find the eigenvalues using the formula $\\det{A-\\lambda I} = 0$\n",
    "* For matrices with unique eigenvalues, solve the system of equations to find the corresponding eigenvectors\n",
    "* For matrices with duplicate eigenvalues, find the constraints on all possible eigenvectors and find a representative one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "\n",
    "[MIT](https://ocw.mit.edu/courses/mathematics/18-03sc-differential-equations-fall-2011/unit-iv-first-order-systems/matrix-methods-eigenvalues-and-normal-modes/MIT18_03SCF11_s33_8text.pdf) <br>\n",
    "[Trinity College Dublin](https://www.scss.tcd.ie/~dahyotr/CS1BA1/SolutionEigen.pdf) <br>\n",
    "[Imperial College London](wwwf.imperial.ac.uk/metric/metric_public/matrices/eigenvalues_and_eigenvectors/eigenvalues2.html\n",
    ") <br>\n",
    "[Oregon State](https://math.oregonstate.edu/home/programs/undergrad/CalculusQuestStudyGuides/vcalc/eigen/eigen.html) <br>\n",
    "[Lamar](http://tutorial.math.lamar.edu/Classes/DE/LA_Eigen.aspx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the inverse matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By definition, the inverse matrix of an m x m matrix is the matrix that returns the identity matrix when multipled by the original matrix:\n",
    "\n",
    "$$A A^{-1} = \\textbf{I} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 2 x 2 matrices, there is a shortcut. If\n",
    "\n",
    "$$A = \n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   a & b\\\\\n",
    "   c & d\\\\\n",
    "  \\end{array} } \\right]$$\n",
    "  \n",
    "Then the inverse matrix $A^{-1}$ is given by:\n",
    "\n",
    "$$A^{-1} = \\frac{1}{ad - bc}\n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   d & -b\\\\\n",
    "   -c & a\\\\\n",
    "  \\end{array} } \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For higher dimensions, you can find the inverse matrix by performing matrix operations on the original matrix A and the identity matrix I until the original matrix is the identity matrix:\n",
    "\n",
    "$$\n",
    "  \\left[ {\\begin{array}{cccccc}\n",
    "   a_{11} & a_{12} & a_{13} & | 1 & 0 & 0\\\\\n",
    "   a_{21} & a_{22} & a_{23} & | 0 & 1 & 0\\\\\n",
    "   a_{31} & a_{32} & a_{33} & | 0 & 0 & 1\\\\\n",
    "  \\end{array} } \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use numpy to find the inverse matrix of our given array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.5    0.25   0.5  ]\n",
      " [ 0.25  -0.875  0.25 ]\n",
      " [ 0.5    0.25  -0.5  ]]\n"
     ]
    }
   ],
   "source": [
    "A1 = np.linalg.inv(A)\n",
    "print(A1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 2 4]\n",
      " [2 0 2]\n",
      " [4 2 3]]\n"
     ]
    }
   ],
   "source": [
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the matrix exponential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix exponential is a convenient way to solve linear homogeneous systems of equations. Suppose we have a system that can be written as:\n",
    "\n",
    "$$\\textbf{X'} = A \\textbf{X} (t) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution can be written as:\n",
    "\n",
    "$$\\textbf{X} (t) = e^{tA} \\textbf{C}$$\n",
    "\n",
    "where $\\textbf{C}$ is an n-dimensional vector of constants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the case of a diagonal matrix, its very easy to define matrix exponential. For example, given the matrix\n",
    "\n",
    "$$A = \n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   3 & 0\\\\\n",
    "   0 & 2\\\\\n",
    "  \\end{array} } \\right]$$\n",
    "  \n",
    "The matrix exponential is\n",
    "\n",
    "$$A = \n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   e^{3t} & 0\\\\\n",
    "   0 & e^{2t}\\\\\n",
    "  \\end{array} } \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this is not the case when your matrix is no longer diagonal. There are a few easy shortcuts you can memorize if your matrix has a specific form. For instance, if your matrix is of the form:\n",
    "\n",
    "$$A = \n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   a & 1\\\\\n",
    "   0 & a\\\\\n",
    "  \\end{array} } \\right]$$\n",
    " \n",
    "Then the matrix exponential will be:\n",
    "\n",
    "$$A = \n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   e^{at} & te^{at}\\\\\n",
    "   0 & e^{at}\\\\\n",
    "  \\end{array} } \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, you may not want to invest all those brain cells on memorizing multiple cases. There is a method for finding the exponential matrix when your matrix is *diagonizable*. A matrix is diagonalizable when there are no repeat eigenvalues. In this specific case, the matrix exponential is given by:\n",
    "\n",
    "$$S e^{Dt} S^{-1} $$\n",
    "\n",
    "where D is the diagonal matrix:\n",
    "\n",
    "$$D = \n",
    "  \\left[ {\\begin{array}{cccc}\n",
    "   \\lambda_1 & 0 & ... & 0\\\\\n",
    "   0 & \\lambda_2 & ... & 0\\\\\n",
    "   \\vdots & \\vdots & & \\vdots \\\\\n",
    "   0 & 0 & ... & \\lambda_n \\\\\n",
    "  \\end{array} } \\right] = S^{-1}AS$$\n",
    "  \n",
    "and S is a matrix where is column is composed of the eigenvectors of matrix A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take, for example, the matrix:\n",
    "\n",
    "$$A = \n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   3 & 5\\\\\n",
    "   1 & -1\\\\\n",
    "  \\end{array} } \\right]$$\n",
    "  \n",
    "We find that the eigenvalues are 4 and -2 with the corresponding eigenvectors (5,1) and (-1, 1), respectively. This gives us the matrices:\n",
    "\n",
    "$$S = \n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   5 & -1\\\\\n",
    "   1 & 1\\\\\n",
    "  \\end{array} } \\right],\n",
    "  S^{-1} = \\frac{1}{6}\n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   1 & 1\\\\\n",
    "   -1 & 5\\\\\n",
    "  \\end{array} } \\right]$$\n",
    "  \n",
    "with a little matrix math,\n",
    "\n",
    "$$e^{At} = \\frac{1}{6}\n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   5e^{4t} + e^{-2t} & 5e^{4t} - 5e^{-2t}\\\\\n",
    "   e^{4t} - e^{-2t} & e^{4t} + 5e^{-2t}\\\\\n",
    "  \\end{array} } \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are unlucky enough to get a non-diagonalizable matrix, then you're going to have a grand time finding the answer. We won't cover that here, but the sources below will give you an idea where to start.\n",
    "\n",
    "Sources:\n",
    "\n",
    "[Math24](https://www.math24.net/method-matrix-exponential/) <br>\n",
    "[Millersville](http://sites.millersville.edu/bikenaga/linear-algebra/matrix-exponential/matrix-exponential.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the singular value decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Singular value decomposition (SVD) is a method to \"simplify\" the information in a given matrix. Unlike the operations explained above, SVD can be performed on rectangular matrices (matrices that aren't m x m).\n",
    "\n",
    "SVD can be used to:\n",
    "\n",
    "* Calculate the pseudoinverse\n",
    "* Perform feature reduction\n",
    "* Perform principle component analysis\n",
    "\n",
    "The SVD for an n x p matrix is calculated by:\n",
    "\n",
    "$$A_{nxp} = U_{nxn}S_{nxp}V^T_{pxp}$$\n",
    "\n",
    "where n is the number of features (columns) and p is the number of observations (rows). To calculate,\n",
    "\n",
    "* The eigenvectors of $AA^T$ make up the columns of U\n",
    "* The eigenvectors of $A^TA$ make up the columns of V\n",
    "* The singular values in S are the square roots of the eigenvalues of $AA^T$ or $A^TA$ in descending order arranged diagonally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:\n",
    "\n",
    "$$A = \n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   1 & 2\\\\\n",
    "   2 & 1\\\\\n",
    "   0 & 0\\\\\n",
    "  \\end{array} } \\right]$$\n",
    "  \n",
    "$AA^T$ is given by:\n",
    "\n",
    "$$AA^T = \n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   1 & 2\\\\\n",
    "   2 & 1\\\\\n",
    "   0 & 0\\\\\n",
    "  \\end{array} } \\right]\n",
    "  \\left[ {\\begin{array}{ccc}\n",
    "   1 & 2 & 0\\\\\n",
    "   2 & 1 & 0\\\\\n",
    "  \\end{array} } \\right] = \n",
    "  \\left[ {\\begin{array}{ccc}\n",
    "   5 & 4 & 0\\\\\n",
    "   4 & 5 & 0\\\\\n",
    "   0 & 0 & 0\\\\\n",
    "  \\end{array} } \\right]\n",
    "  $$\n",
    "\n",
    "The eigenvalues of $AA^T$ are found to be 9, 1, and 0, with the corresponding eigenvectors (1,1,0), (1,-1,0), and (0,0,1). This gives the matrix U (after normalization):\n",
    "\n",
    "$$U = \\frac{1}{\\sqrt{2}}\n",
    "  \\left[ {\\begin{array}{ccc}\n",
    "   1 & 1 & 0\\\\\n",
    "   1 & -1 & 0\\\\\n",
    "   0 & 0 & 1\\\\\n",
    "  \\end{array} } \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$A^TA$ is given by:\n",
    "\n",
    "$$AA^T = \n",
    "  \\left[ {\\begin{array}{ccc}\n",
    "   1 & 2 & 0\\\\\n",
    "   2 & 1 & 0\\\\\n",
    "  \\end{array} } \\right]\n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   1 & 2\\\\\n",
    "   2 & 1\\\\\n",
    "   0 & 0\\\\\n",
    "  \\end{array} } \\right] = \n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   5 & 4\\\\\n",
    "   4 & 5\\\\\n",
    "   \\end{array} } \\right]\n",
    "  $$\n",
    "\n",
    "The eigenvalues of $A^TA$ are found to be 9, and 1, with the corresponding eigenvectors (1,1) and (1,-1). This gives the matrix V (after normalization):\n",
    "\n",
    "$$V = \\frac{1}{\\sqrt{2}}\n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   1 & 1\\\\\n",
    "   -1 & 1\\\\\n",
    "  \\end{array} } \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the S matrix is given by\n",
    "\n",
    "$$S = \n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   3 & 0\\\\\n",
    "   0 & 1\\\\\n",
    "   0 & 0\\\\\n",
    "  \\end{array} } \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same thing computationally taking advantage of the `numpy` package `svd`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U: [[-0.70710678 -0.70710678  0.        ]\n",
      " [-0.70710678  0.70710678  0.        ]\n",
      " [ 0.          0.          1.        ]]\n",
      "\n",
      "S: [[3. 0.]\n",
      " [0. 1.]\n",
      " [0. 0.]]\n",
      "\n",
      "V: [[-0.70710678  0.70710678]\n",
      " [-0.70710678 -0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1,2],[2,1],[0,0]])\n",
    "U, S, VT = np.linalg.svd(A)\n",
    "Si = np.zeros(A.shape)\n",
    "Si[0:S.shape[0], 0:S.shape[0]] = np.identity(S.shape[0]) * S\n",
    "V = np.transpose(VT)\n",
    "UT = np.transpose(U)\n",
    "\n",
    "print('U: {}\\n'.format(U))\n",
    "print('S: {}\\n'.format(Si))\n",
    "print('V: {}'.format(V))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if SVD holds up to its claims. We should be able to recover the original matrix using the U, S, and V matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2.]\n",
      " [2. 1.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(np.dot(U,Si),VT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudoinverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVD transform can also be used to calculate a matrix known as the pseudoinverse. The psuedoinverse, like its counterpart, the inverse, returns the identity matrix when multiplied by the original matrix:\n",
    "\n",
    "$$A A^{\\oplus} = I $$\n",
    "\n",
    "The pseudoinverse operation, however, doesn't require the original matrix to be square. If the original matrix is of dimensions m x n, then the dimensions of the pseudomatrix are n x m. The pseudomatrix is calculated by:\n",
    "\n",
    "$$A^{\\oplus} = V S^{\\oplus} U^T $$\n",
    "\n",
    "But uh oh. The calculation of the pseudomatrix requires that we know the pseudomatrix of the singular matrix. How do we get that? Well, since the singular matrix is a diagonal matrix, you simply invert the scalar values of the entries e.g.:\n",
    "\n",
    "$$S = \n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   3 & 0\\\\\n",
    "   0 & 1\\\\\n",
    "   0 & 0\\\\\n",
    "  \\end{array} } \\right],\n",
    "  S^{\\oplus} = \n",
    "  \\left[ {\\begin{array}{ccc}\n",
    "   \\frac{1}{3} & 0 & 0\\\\\n",
    "   0 & 1 & 0\\\\\n",
    "  \\end{array} } \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pseudoinverse of A:\n",
      "[[-0.33333333  0.66666667  0.        ]\n",
      " [ 0.66666667 -0.33333333  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "Di = np.zeros(np.transpose(A).shape)\n",
    "Di[0:S.shape[0], 0:S.shape[0]] = np.linalg.inv( np.identity(S.shape[0]) * S)\n",
    "Ai = np.dot(np.dot(V,Di), UT)\n",
    "print('pseudoinverse of A:\\n{}'.format(Ai))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that our pseudoinverse is working correctly if it returns the identity matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is this the identity matrix?:\n",
      "[[ 1.00000000e+00 -3.33066907e-16  0.00000000e+00]\n",
      " [ 2.22044605e-16  1.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print('Is this the identity matrix?:\\n{}'.format(np.dot(A,Ai)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two elements of the diagonal correspond to the elements of the identity matrix, but the last one doesn't (it's zero). This is due to the fact that our original matrix had an empty row. So we *almost* get a full identity matrix. Another reason why it's called the pseudoinverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common problem in machine learning methods is the need to reduce the dimensionality of a dataset e.g. you have more features (columns) than observations (rows). It is possible to reduce your feature set into a smaller set of features most relevant to the problem. There are multiple approaches to do this, and SVD is one of them.\n",
    "\n",
    "To do this, we take a subset of the singular values in S and V:\n",
    "\n",
    "$$B = U S_k V^T_k$$\n",
    "\n",
    "This doesn't work well with our previous example, because we only had two columns or features.\n",
    "\n",
    "SVD is also used as a substep in more complicated dimensionality reduction methods like PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "\n",
    "[machine learning mastery](https://machinelearningmastery.com/singular-value-decomposition-for-machine-learning/) <br>\n",
    "[MIT](http://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm) <br>\n",
    "[Carnegie Mellon](https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/book-chapter-4.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
